<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Artificial Intelligence | Shivansh Mundra</title>
    <link>https://shivanshmundra.github.io/categories/artificial-intelligence/</link>
      <atom:link href="https://shivanshmundra.github.io/categories/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <description>Artificial Intelligence</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Shivansh Mundra</copyright><lastBuildDate>Tue, 18 Aug 2020 14:24:26 +0530</lastBuildDate>
    <image>
      <url>https://shivanshmundra.github.io/img/icon-192.png</url>
      <title>Artificial Intelligence</title>
      <link>https://shivanshmundra.github.io/categories/artificial-intelligence/</link>
    </image>
    
    <item>
      <title>Multi Modal Variational AutoEncoders</title>
      <link>https://shivanshmundra.github.io/paper_summary/mmvae/</link>
      <pubDate>Tue, 18 Aug 2020 14:24:26 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/mmvae/</guid>
      <description>&lt;h1 id=&#34;variational-mixture-of-experts-autoencoders-for-multi-modal-deep-generative-modelshttpsarxivorgpdf191103393pdf&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1911.03393.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Variational Mixture-of-Experts Autoencoders for Multi Modal Deep Generative Models&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;They characterize successful learning of such models as fulfilment of four criteria - i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818150052698.png&#34; alt=&#34;image-20200818150052698&#34;&gt;&lt;/p&gt;
&lt;p&gt;When we take all modalities(visual, linguistic and physical) into account, we can get a better understanding and representation of context, for ex. here bird.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818150435770.png&#34; alt=&#34;image-20200818150435770&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818150445361.png&#34; alt=&#34;image-20200818150445361&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Latent Factorization - There is some shared and private spaces across different modalities.&lt;/li&gt;
&lt;li&gt;Joint Generation - From union of both spaces we could generate different modalities. Basically, for example, text and image should be semantically same.&lt;/li&gt;
&lt;li&gt;Cross Generation - Model can generate data in one modality conditioned on some other modality. For ex. From text, generate an image.&lt;/li&gt;
&lt;li&gt;Synergy - Observing both modalities should enhance context understanding. For example observing image and text should improve in specificity of generation of image and text when taken alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818151150020.png&#34; alt=&#34;image-20200818151150020&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818151230522.png&#34; alt=&#34;image-20200818151230522&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;related-works&#34;&gt;Related Works&lt;/h3&gt;
&lt;p&gt;There has been some work in the area of joint representations for example&lt;/p&gt;
&lt;p&gt;Suzuki et al. (2017) introduced the joint multimodal VAE (JMVAE) that learns shared representation with joint encoder qΦ(z | x1, x2). To handle missing data at test time, two unimodal encoders qΦ(z | x1) and qΦ(z | x2) are trained to match qΦ(z | x1, x2) with a KL constraint between them.&lt;/p&gt;
&lt;p&gt;However, they only focuses on one way transformation that too image-to-image and often require additional modelling components.&lt;/p&gt;
&lt;p&gt;More recently, Wu and Goodman (2018) introduced Product of Experts(PoE) over marginal posteriors, enabling cross model generations at test time without requiring additional inference networks and multi-stage training regimes.&lt;/p&gt;
&lt;h3 id=&#34;some-mathematics&#34;&gt;Some Mathematics&lt;/h3&gt;
&lt;p&gt;We denote modalities with m = 1,2,3&amp;hellip;&amp;hellip;M and latent representation as z.&lt;/p&gt;
&lt;p&gt;VAE is of form :  &lt;img src=&#34;image-20200818152456926.png&#34; alt=&#34;image-20200818152456926&#34;&gt;&lt;/p&gt;
&lt;p&gt;VAEs are parametrized by $\theta$ which is deep neural network.&lt;/p&gt;
&lt;p&gt;The objective of training VAEs is to maximise the marginal likelihood of
the data pΘ(x1:M). However, computing the evidence is intractable as it requires knowledge of the true joint posterior pΘ(z | x1:M). To tackle this, we approximate the true unknown posterior by a variational posterior qΦ(z | x1:M), which now allows optimising an evidence lower bound (ELBO) through stochastic gradient descent (SGD), with ELBO defined as&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200819173629938.png&#34; alt=&#34;image-20200819173629938&#34;&gt;&lt;/p&gt;
&lt;p&gt;They further used IWAE estimator to get a tighter bound and higher entropy.&lt;/p&gt;
&lt;p&gt;Here too, we are facing one issue, how to get joint posterior q$\phi$? One basic way is get a single encoder that takes all modalities as input. But that would mean, we need all modalities present at test time which won&amp;rsquo;t be true for cross modal generation.&lt;/p&gt;
&lt;p&gt;So instead they propose to factorise joint variational posterior as a combination of unimodal posteriors(weighted sum) :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200819174049519.png&#34; alt=&#34;image-20200819174049519&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When using Product-of-Experts, any one modality hold the power of veto.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For better understanding of VAEs both from Neural Network and probabilistic perspective, you can follow these links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ermongroup.github.io/cs228-notes/inference/variational/&#34;&gt;https://ermongroup.github.io/cs228-notes/inference/variational/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter19/readings/VI_Review.pdf&#34;&gt;https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter19/readings/VI_Review.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&#34;&gt;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;As this is a new idea and theory based paper, they have done experiments on MNIST-SVHN data and Caltech Bird dataset(challenging). You can view results 
&lt;a href=&#34;https://github.com/iffsid/mmvae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; on all four characteristics mentioned above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Deep Learning requires Re-Thinking Generalisation</title>
      <link>https://shivanshmundra.github.io/paper_summary/rethinking_generalisation/</link>
      <pubDate>Mon, 22 Jun 2020 09:15:50 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/rethinking_generalisation/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1611.03530&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;generalisation-in-machine-learning-models&#34;&gt;Generalisation in Machine Learning Models&lt;/h3&gt;
&lt;p&gt;In Supervised Machine Learning, we train the model on training set and evaluate the model on a validation/testing set which is unseen during the training process. It is assumed that the data distribution is same for training and testing set.&lt;/p&gt;
&lt;p&gt;So, basically our goal for a &lt;strong&gt;good&lt;/strong&gt; Machine Learning model is to &lt;strong&gt;generalise&lt;/strong&gt; well from training data to any data from the problem domain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;generalisation&lt;/strong&gt; refers to how well a machine learning model learns and generalises to a new data avoiding Over-fitting and Under-fitting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;generalisation Error&lt;/strong&gt; refers to difference between Training error and Testing error. Increases with over fitting.&lt;/p&gt;
&lt;h2 id=&#34;key-findings-from-paper&#34;&gt;Key Findings from Paper&lt;/h2&gt;
&lt;p&gt;As of now, we understand that if a model is big(large parameter space), it tries to over-fit training data and we need explicit regularisation(l2 norm on parameters, data augmentations, weight decay, Early Stopping) to decrease Generalisation error.
&lt;strong&gt;But&lt;/strong&gt; this paper has shown that SOTA image classification models easily fit on random labelling of training data or even when true pixels are replaced by random pixels. This process is unaffected by explicit regularisation.&lt;/p&gt;
&lt;p&gt;By their experiments, they show that if size of Deep Neural Networks is greater than sample size(which usually happens), Neural Networks are able to capture meaningful signal in the data, &lt;strong&gt;while at the same time fit the noisy part using brute-force.&lt;/strong&gt; This is also unaffected by hyper-parameter tuning, same set of hyper-parameters converges for noisy labelled data too.&lt;/p&gt;
&lt;h3 id=&#34;effective-capacity-of-neural-network&#34;&gt;Effective Capacity of Neural Network&lt;/h3&gt;
&lt;p&gt;They have shown with their experiments that generalisation error not only depend on explicit regularisation and optimizer but on model architectures as well.&lt;/p&gt;
&lt;p&gt;Also, random noisy image pixels are easy to over-fit compared to true images as random images are well separated in pixel space than original one.&lt;/p&gt;
&lt;h3 id=&#34;traditional-bound-for-generalization&#34;&gt;Traditional bound for Generalization&lt;/h3&gt;
&lt;p&gt;Theoretically, we have VC dimension and Rademacher complexity to measure capacity of Classification model. But authors from their results showed that regularisation bound achieved theoretically is of no use in realistic settings.&lt;/p&gt;
&lt;p&gt;Similar is the case with evaluation of robustness of training algorithms. Namely Uniform Stability, which doesn&amp;rsquo;t take data into account just algorithm and it&amp;rsquo;s difficult to utilise this stability notion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;finally-role-of-regularisation&#34;&gt;Finally Role of Regularisation&lt;/h2&gt;
&lt;p&gt;With experimenting on different architectures on namely 3 explicit regularisation - data augmentation, weight decay and Dropout authors concluded that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularisation is important, but we can achieve more by just changing the architecture of model.&lt;/li&gt;
&lt;li&gt;Regularises are not only responsible for generalisation capacity of deep nets.&lt;/li&gt;
&lt;li&gt;Early Stopping could potentially improve generalisation performance.&lt;/li&gt;
&lt;li&gt;Batch Norm give marginal gains on generalisation error.&lt;/li&gt;
&lt;li&gt;Model could perform well even after regularises are removed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;expressive-power-of-nns&#34;&gt;Expressive Power of NNs&lt;/h3&gt;
&lt;p&gt;Authors chose a different approach than traditional approach of showing what functions of entire domain can or can&amp;rsquo;t be represented. They show expressive power on a finite sample of size $n$.&lt;/p&gt;
&lt;p&gt;In their words, &amp;ldquo;There exists a two-layer neural network with ReLU activation and $2n+d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;implicit-regularisation&#34;&gt;Implicit Regularisation&lt;/h3&gt;
&lt;p&gt;They have theoretically shown that in linear models, SGD often converge to solution with minimum norm. And minimum norm doesn&amp;rsquo;t predict anything about generalisation performance.&lt;/p&gt;
&lt;p&gt;Doing simple tricks like preprocessing with some conv layers, gaussian kernels on pixels improve generalisation error which pose serious questions on traditional generalisation understanding.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This shows that the reasons for why optimization is empirically easy must be different from the true cause of generalisation&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recurrent Multimodal Interaction for Referring Image Segmentation</title>
      <link>https://shivanshmundra.github.io/paper_summary/recurrent_multimodal/</link>
      <pubDate>Wed, 29 Apr 2020 00:17:44 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/recurrent_multimodal/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;The problem statement is to segment out visual object from natural language query. Semantics of that visual object are defined from NLP query. For ex. &lt;code&gt;Man in pink shirt&lt;/code&gt; in some image, we want to segment out corresponding region where any/all man is wearing pink shirt.&lt;/p&gt;
&lt;h2 id=&#34;related-works-and-inspiration&#34;&gt;Related Works and Inspiration&lt;/h2&gt;
&lt;p&gt;Right now this problem is addressed in academia by extracting out image features separately from Deep Convolution Neural Network and language features from LSTM cells. These features are then concatenated together to form a fused representation of image and query. This fused representation is somehow used to predict corresponding segmented region with trivial loss function.&lt;/p&gt;
&lt;p&gt;What authors feel is we, human don&amp;rsquo;t perceive things is similar manner like LSTM, we don&amp;rsquo;t remember everything after the last loop over word embeddings. In author words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, consider the expression “the man on the right wearing blue”. Without seeing an actual image, all information in the sentence needs to be remembered, meaning the sentence embedding needs to encode IS MAN, ON RIGHT, WEAR BLUE jointly. However, with the actual image available, the reasoning process can be decomposed as a sequential process, where the model first identifies all pixels that agree with IS MAN, then prunes out those that do not correspond with ON RIGHT, and finally suppresses those that do not agree with WEAR BLUE.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This task is also similar to visual question answering task where you need to generate answer(NLP) based on an image and a question. The difference only is, in VQA, we need sequence generation(in answer) while here we need to produce segmentation mask at once.&lt;/p&gt;
&lt;p&gt;Authors achieve this by applying LSTM in a convolution manner.&lt;/p&gt;
&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We propose a novel model, namely convolutional multimodal LSTM, to encode the sequential interactions between individual semantic, visual, and spatial information.&lt;/li&gt;
&lt;li&gt;We demonstrate the superior performance of the word to-image multimodal LSTM approach on benchmark datasets over the baseline model.&lt;/li&gt;
&lt;li&gt;We analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-toimage interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Basically they want to convert this referring image segmentation into an sequential process.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;rmi_model.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;rmi_np.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here concatenation of visual features and spatial features with language features is fed to &lt;code&gt;mLSTM&lt;/code&gt; cell as an input and &lt;code&gt;mLSTM&lt;/code&gt; cell produces an output for same. SInce mLSTM receives multimodal data, we can understand this as a 1x1 convolution applied to fused features .&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The same mLSTM operation is shared for all image locations. This is equivalent to treating the mLSTM as a 1×1 convolution over the feature map of size W 0 × H0 × (DI + DS + 8). In other words, this is a convolutional LSTM that shares weights both across spatial location and time step.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;result_rmi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-takeaways&#34;&gt;My Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This idea of treating this problem as sequential problem is innovative and having an mLSTM cell to encode both visual and linguistic features is also good. This gives model ability to forget all those pixel which defy correspondence initially.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;My guts are that this kind of model would work good even where there are small objects because at each time step there would be a reduction/change of probable pixels for segmentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I still believe that this essentially is fusing of features from image and language just more diffused!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I want to study good papers on VQA as they can provide me with good literature and insights on this task.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>My Journey to Natural Language Processing</title>
      <link>https://shivanshmundra.github.io/paper_summary/intro_nlp/</link>
      <pubDate>Sat, 25 Apr 2020 11:59:39 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/intro_nlp/</guid>
      <description>&lt;h2 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h2&gt;
&lt;p&gt;I consider the reader to be familiar with normal machine learning terminology and methods. We here are going to deal with Supervised learning mostly.&lt;/p&gt;
&lt;p&gt;Since all machine learning models are mathematical function approximators, we can;t input a sentence to a mathematical model!&lt;/p&gt;
&lt;p&gt;There is nothing like &lt;code&gt;f(&#39;I am here to learn NLP&#39;)&lt;/code&gt;, one need to convert from our language to machine language so that it becomes something like &lt;code&gt;f([1, 0, 1, 2, 5 ..])&lt;/code&gt;, since this is the method computer is most familiar with. So first jumping to this conversion:&lt;/p&gt;
&lt;h3 id=&#34;conversion-of-text-to-mathematical-representation&#34;&gt;Conversion of text to mathematical representation&lt;/h3&gt;
&lt;h4 id=&#34;1-basic-approach-of-making-dictionary&#34;&gt;1. Basic Approach of making dictionary&lt;/h4&gt;
&lt;p&gt;Here you just create a dictionary or set of all available words present in data and represent each word/sentence with one hot vector. For example : Let&amp;rsquo;s say we have 8 words in our dictionary and sentence &lt;code&gt;I like banana&lt;/code&gt; is to be encoded, it would be something like &lt;code&gt;[1,0,0,0,1,0,0,1]&lt;/code&gt; where index representing &lt;code&gt;I, like and Banana&lt;/code&gt;. In this case, length of this vector would be fixed and equal to vocabulary of data.&lt;/p&gt;
&lt;p&gt;In this case, there is no sense of context here, &lt;code&gt;like&lt;/code&gt; can be used in different context yet same representation.  But duh! It&amp;rsquo;s the first method and supposed to be this dumb!&lt;/p&gt;
&lt;h4 id=&#34;2-tf-idf-representation&#34;&gt;2. TF-IDF Representation&lt;/h4&gt;
&lt;p&gt;Full form is : &lt;strong&gt;Term Frequency-Inverse Document Frequency&lt;/strong&gt; . The full form mentioned is self explanatory still we would want to get a feel. So here it is:&lt;/p&gt;
&lt;p&gt;Term-Frequency representation means number of times a word is occurring and represent in one hot vector by it&amp;rsquo;s frequency for ex &lt;code&gt;[1,2,0,0,3]&lt;/code&gt;. Now in this case, let&amp;rsquo;s say there is a common word &lt;code&gt;the&lt;/code&gt; which is occurring in most of corpus(another name for sentence!) but adds no specific meaning to understanding. So we take TF representation and multiply it with IDF which is $log(N/nw)$ where N is total number of document/corpus  and nw is number of corpus which contain that word. Smart thinking!&lt;/p&gt;
&lt;p&gt;We won&amp;rsquo;t use this in our supervised learning paradigm, we would just go with one hot vector and prepare embedding for our model.&lt;/p&gt;
&lt;h3 id=&#34;terminologies&#34;&gt;Terminologies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Corpus(singular- Corpura) : A sentence or document in general consisting of words.&lt;/li&gt;
&lt;li&gt;Tokenization: Splitting a corpus based on white spaces(in general) in between(for English specifically).
&lt;ul&gt;
&lt;li&gt;Say a sentence &lt;code&gt;I want to learn Tokenization :-)&lt;/code&gt; is to be tokenized.&lt;/li&gt;
&lt;li&gt;Tokenization would be &lt;code&gt;[&#39;I&#39;, &#39;want&#39;, &#39;to&#39;, &#39;learn&#39;, &#39;Tokenization&#39;, &#39;:-)&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;There are several libraries which use context and tokenize the corpus.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;N-gram tokens: Taking consecutive &lt;code&gt;n&lt;/code&gt; tokens from tokenized representation in contiguous manner.&lt;/li&gt;
&lt;li&gt;POS Tagging: Categorising tokens to parts of speech.&lt;/li&gt;
&lt;li&gt;Dependency Parsing: Extracting relationship/dependency between different tokens in sentence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;workflow-of-nlp-pipeline&#34;&gt;Workflow of NLP Pipeline&lt;/h3&gt;
&lt;p&gt;In every NLP data, there are three main components:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Vocabulary:&lt;/strong&gt; We want a bijection mapping where we map tokens to mathematical integers. We create a dictionary mapping tokens to indexes. This would also contain index to token mapping.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Vectorizer&lt;/strong&gt;: This method produces vectors from text/corpus using vocabulary above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Dataloader&lt;/strong&gt;: Traditional data science batchloader which yield data points in rolling fashion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xception: Deep Learning with Depthwise Separable Convolutions</title>
      <link>https://shivanshmundra.github.io/paper_summary/xceptionnet/</link>
      <pubDate>Wed, 22 Apr 2020 21:19:38 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/xceptionnet/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1610.02357&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;previous-works-and-inspiration&#34;&gt;Previous Works and Inspiration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;While Inception modules are conceptually similar to convolutions, the empirically appear to be capable of learning richer representations with less parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A convolution layer attempts to learn filters in a 3D space, with 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In Inception module, we introduce 1x1 convolutions instead of 3 or 5 to extract fewer channels and focus on spatial information. After that mapping all correlations in these smaller 3D spaces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In theory, cross channel correlations and spatial correlations are decoupled in Inception module.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;inception_archi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;separable-convolution-layer&#34;&gt;Separable Convolution Layer&lt;/h3&gt;
&lt;p&gt;So essentially the idea of inception of separating cross channel mapping and spatial mapping was already there as a layer in Tensorflow as &amp;ldquo;Separable Conv&amp;rdquo; Layer.&lt;/p&gt;
&lt;p&gt;Main difference between this and Inception version is :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The order of the operations: depthwise separable convolutions as usually  implemented (e.g. in TensorFlow) perform first channel-wise spatial convolution and then perform 1x1 convolution, whereas Inception performs the 1x1 convolution first.(doesn&amp;rsquo;t have a difference as both are stacked)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The presence or absence of a non-linearity after the first operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise separable convolutions are usually implemented without non-linearities.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Author say that we can totally decouple cross channel correlations and spatial correlations in the feature maps of convolution neural network.&lt;/li&gt;
&lt;li&gt;Parameters are almost same in Inception and Xception model while Xception outperforms by a large margin on a dataset with large number of classes(17k).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;xception_archi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rest all things are same, just replacing inception module by depthwise separable conv layer block also making the architecture simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;takeaway&#34;&gt;Takeaway&lt;/h2&gt;
&lt;p&gt;Idea of decoupling of channel dimension and spatial feature mapping from convolution kernel is great! So deciphering it mathematically:&lt;/p&gt;
&lt;p&gt;We know each channel in Convolution block represents some high level feature say in human, each channel would map a body part(just for example!). Then this idea of  depthwise conv proposes that in process of identifying one body part don&amp;rsquo;t process info from other body part info, it would disturb the signal in processing.&lt;/p&gt;
&lt;p&gt;After we get good signal info body part, then we can use info from others to get high level part info.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Imitation Learning: From Why to How!</title>
      <link>https://shivanshmundra.github.io/post/imitation_learning/</link>
      <pubDate>Sun, 19 Apr 2020 02:15:34 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/post/imitation_learning/</guid>
      <description>&lt;p&gt;Jude is an amateur football player but a wannabe great player. Jude is a die-hard fan of Christiano Ronaldo and watches every time he is playing anywhere.&lt;/p&gt;
&lt;p&gt;Jude wondered if he could exactly be like Ronaldo. If by humongous practice, he could imitate all skills Ronaldo has. He doesn&amp;rsquo;t care that while imitating if he is scoring a goal or not, he would just kick as Ronaldo would have kicked.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If I would kick exactly like Ronaldo, it would definitely be a goal!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Jude was fascinated by this idea, but also he was &lt;strong&gt;too lazy&lt;/strong&gt; and &lt;strong&gt;loved programming&lt;/strong&gt;(eternal bond). He thought why couldn’t he do something very similar in the virtual world, after all, he could fast forward this training process as he controls this virtual world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shivanshmundra.github.io/img/football.jpeg&#34; alt=&#34;Football image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Jude stumbled upon exactly what he was looking for&lt;/p&gt;
&lt;h3 id=&#34;imitation-learning&#34;&gt;Imitation Learning&lt;/h3&gt;
&lt;p&gt;Continuing Jude&amp;rsquo;s story, as he watches Ronaldo a lot, he knows what he would do in numerous situations thrown at him. Let’s say Jude has practised and can mimic exactly Ronaldo in those &lt;strong&gt;exactly the same situations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As the number of situations Ronaldo(or any football player) can be in is infinite, Jude can’t learn on all of them, how hard he tries! And since Jude was so busy in replicating Ronaldo, he didn’t learn anything when an unknown situation arises.&lt;/p&gt;
&lt;p&gt;Also, since Jude is human just like us, there is nothing like &lt;strong&gt;exactly like&lt;/strong&gt; Ronaldo. However hard he try, there would be some scope of mistake every time!&lt;/p&gt;
&lt;p&gt;We would see above circumstances projected mathematically later.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can be wrong, or we can know it, but we can’t do both at the same time&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you understood Jude, you got Imitation learning. 😄&lt;/p&gt;
&lt;h3 id=&#34;different-learning-paradigms&#34;&gt;Different Learning Paradigms&lt;/h3&gt;
&lt;p&gt;Let us quickly define three verticals of machine learning which would help define our problem statement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning: Basically Find x&lt;sub&gt;s&lt;/sub&gt; and y&lt;sub&gt;s&lt;/sub&gt;, define a model and fit model to predict y&lt;sub&gt;s&lt;/sub&gt;.&lt;/li&gt;
&lt;li&gt;Unsupervised learning: We wish to inherently learn the structure of data without &lt;strong&gt;explicitly&lt;/strong&gt; using labels.&lt;/li&gt;
&lt;li&gt;Reinforcement learning: It&amp;rsquo;s about taking suitable actions in order to maximize reward in an environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;now-why-imitation-learning&#34;&gt;Now Why Imitation Learning?&lt;/h3&gt;
&lt;p&gt;Now consider that Jude won&amp;rsquo;t use Imitation learning as of now and want to train himself for every situation with objective being shooting a goal using Reinforcement learning.&lt;/p&gt;
&lt;p&gt;In RL setting, reward for shooting a goal would be &lt;strong&gt;+1&lt;/strong&gt; and for rest it would be &lt;strong&gt;0&lt;/strong&gt;. As you know, this kind of reward is very sparse in nature. There may be handful of time where agent(Jude in our case!) would receive a positive reward to learn from.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The reward sparsity is one of the issues that hinders us in beating Montezuma’s Revenge — a notoriously hard Atari 2600 game, that has not been cracked yet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another issue is one we discussed above, we need virtual simulators to fasten this process of learning but what if we don&amp;rsquo;t have one or we want to learn something in real life?&lt;/p&gt;
&lt;p&gt;Imitation learning can potentially solve these problems and also a bunch of others.&lt;/p&gt;





  











&lt;figure id=&#34;figure-virtual-humanoid-trying-to-imitate-human-in-running&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shivanshmundra.github.io/img/humanoid_imitation.gif&#34; data-caption=&#34;Virtual Humanoid trying to imitate Human in running&#34;&gt;


  &lt;img src=&#34;https://shivanshmundra.github.io/img/humanoid_imitation.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Virtual Humanoid trying to imitate Human in running
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;how-imitation-learning&#34;&gt;How Imitation learning?&lt;/h3&gt;
&lt;p&gt;Generally, Imitation learning is useful when it is easier for an expert to demonstrate the desired behaviour so that we don&amp;rsquo;t have to specify any reward function.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say we collect expert demonstrations (also known as trajectories in RL setting)&lt;/p&gt;
&lt;p&gt;$\tau$ = (s&lt;sub&gt;0&lt;/sub&gt;, a&lt;sub&gt;0&lt;/sub&gt;, s&lt;sub&gt;1&lt;/sub&gt;, a&lt;sub&gt;1&lt;/sub&gt;&amp;hellip;..) where actions (A&lt;sub&gt;s&lt;/sub&gt;) are based on expert&amp;rsquo;s policy(say human brain).  In some cases, we may require presence of &amp;ldquo;&lt;strong&gt;expert&lt;/strong&gt;&amp;rdquo; during training.&lt;/p&gt;
&lt;p&gt;Once we get this trajectory, we slice this on each time step to get pairs of S&lt;sub&gt;s&lt;/sub&gt; and A&lt;sub&gt;s&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;We then treat these pairs as i.i.d examples and apply &lt;strong&gt;Supervised learning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Changing loss function and optimisation strategies in this learning define various imitation learning algorithms. Let&amp;rsquo;s look at basic ones in them.&lt;/p&gt;
&lt;h3 id=&#34;behaviour-cloning&#34;&gt;Behaviour Cloning&lt;/h3&gt;
&lt;p&gt;This is simplest form of imitation learning where you treat these pairs(S&lt;sub&gt;s&lt;/sub&gt;, A&lt;sub&gt;s&lt;/sub&gt;) as i.i.d examples and apply simple supervised learning. We can choose the model to a tree based (Random Forest, Gradient Boosting) or Neural Networks depending on complexity of state and action space.&lt;/p&gt;
&lt;h6 id=&#34;enough-of-talks-lets-see-implementation&#34;&gt;Enough of talks, let&amp;rsquo;s see implementation!&lt;/h6&gt;
&lt;p&gt;You can head over to this folder : &lt;a href=&#34;https://github.com/Shivanshmundra/reinforcement_learning_beginner/tree/master/imitation_learning_bc&#34;&gt;https://github.com/Shivanshmundra/reinforcement_learning_beginner/tree/master/imitation_learning_bc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, you can run &lt;code&gt;expert_recorder.py&lt;/code&gt; to record expert demonstrations, expert being you!. Right now it uses &lt;code&gt;MountainCar-v0&lt;/code&gt; environment where you have only 2 actions - right or left, to climb a mountain. You can guide by using &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; for right and left respectively. You can change environment according to your ease too!&lt;/p&gt;
&lt;p&gt;Once you have created &amp;ldquo;expert demonstrations&amp;rdquo;,  you can build a model and train on collected data. Trained policy then can be used to test on same environment.&lt;/p&gt;
&lt;p&gt;Extensive instructions are in &lt;code&gt;README&lt;/code&gt; of folder. Also, I have borrowed this code, so to get good understanding of code, you can watch 
&lt;a href=&#34;https://www.youtube.com/watch?v=0rsrDOXsSeM&amp;amp;feature=youtu.be&amp;amp;t=1370&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; video.&lt;/p&gt;
&lt;p&gt;Once trained, it looks something like this:&lt;/p&gt;





  











&lt;figure id=&#34;figure-mountain-car-trained-using-behaviour-cloning&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shivanshmundra.github.io/img/imitation_learning.gif&#34; data-caption=&#34;Mountain Car trained using Behaviour Cloning&#34;&gt;


  &lt;img src=&#34;https://shivanshmundra.github.io/img/imitation_learning.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Mountain Car trained using Behaviour Cloning
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h5 id=&#34;shortcomings&#34;&gt;Shortcomings&lt;/h5&gt;
&lt;p&gt;In some cases, Behavioural cloning can work excellently, precisely where there are limited number of states and time frame of an episode is very short.&lt;/p&gt;
&lt;p&gt;Main reason for Behaviour cloning to not work generally is the i.i.d assumption we took above for supervised learning, while in real scenario like an MDP(Markov Decision Process) an action induces the next state, which breaks i.i.d assumption.&lt;/p&gt;
&lt;p&gt;Also, as you know this algorithms is only trained on given states as we previously talked that no machine learning algorithm is 100% accurate, there is a chance of error on some time step $t$ and this error will keep on increasing with $t$ as with every wrong decision, agent might fall into an &amp;ldquo;more unknown&amp;rdquo; state thus prone to make more error.&lt;/p&gt;
&lt;p&gt;In above cases, BC behaviour is undefined and can lead to catastrophic failures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://shivanshmundra.github.io/img/failure_bc.png&#34; alt=&#34;image-20200419033525905&#34;&gt;&lt;/p&gt;
&lt;p&gt;​					&lt;em&gt;Source : &lt;a href=&#34;https://web.stanford.edu/class/cs234/slides/lecture7.pdf&#34;&gt;https://web.stanford.edu/class/cs234/slides/lecture7.pdf&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;dagger-dataset-aggregation-algorithm&#34;&gt;DAgger (Dataset Aggregation) Algorithm&lt;/h3&gt;
&lt;p&gt;This is basically an improved version of Behaviour cloning algorithms we discussed above.&lt;/p&gt;
&lt;p&gt;Primary shortcoming of BC was that the data it had for training was static and agent could lead to unknown states where it doesn&amp;rsquo;t have expert demonstrations.&lt;/p&gt;
&lt;p&gt;We could improve that by keeping expert in loop of training and querying expert in each loop to collect more data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram
    Expert_labelling_new_data --&amp;gt; Policy_learning_from_new_data
    Policy_learning_from_new_data --&amp;gt; Sampling_data_from_learned_policy
    Sampling_data_from_learned_policy --&amp;gt; Expert_labelling_new_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The DAgger algorithms works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initial policy: $\pi$ &lt;sub&gt;0&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;For m = 1:
&lt;ul&gt;
&lt;li&gt;Collect trajectories $\tau$ sampled from policy $\pi$&lt;sub&gt;m-1&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;Estimate state distribution by discarding actions from $\tau$.&lt;/li&gt;
&lt;li&gt;Ask expert to label/feedback on those states interactively.&lt;/li&gt;
&lt;li&gt;Creating union of data collected till now  $\tau$&lt;sub&gt;1,2,&amp;hellip;m&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;Training $\pi$ on this union of data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DAgger is very efficient method, which doesn&amp;rsquo;t have shortcomings of Behaviour cloning algorithm. The only limitation of this method is the fact, that we need an expert that can evaluate the agent’s actions at all times, which is not possible in some applications.&lt;/p&gt;
&lt;h4 id=&#34;lets-see-implementation-of-dagger&#34;&gt;Let&amp;rsquo;s see implementation of DAgger&lt;/h4&gt;
&lt;p&gt;I have used code from Sergey Levine&amp;rsquo;s 
&lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS285 Deep Reinforcement Learning course&lt;/a&gt; material. If you want to dive deep into Deep Reinforcement learning, it&amp;rsquo;s a gold  mine!&lt;/p&gt;
&lt;p&gt;Head over to my github repository - &lt;a href=&#34;https://github.com/Shivanshmundra/CS285-DeepRL-Solutions/tree/master/hw1&#34;&gt;https://github.com/Shivanshmundra/CS285-DeepRL-Solutions/tree/master/hw1&lt;/a&gt; for DAgger implementation on different gym environments.&lt;/p&gt;
&lt;p&gt;In this solution, instead of human expert labelling state space, we have an expert policy which when queried, induce actions on given states. You can see with every iteration of Data Aggregation, episodic reward increases and agent approach towards convergence.&lt;/p&gt;
&lt;p&gt;This repository also contain implementation of Behaviour cloning to compare between BC and DAgger algorithm.&lt;/p&gt;
&lt;p&gt;Some of trained policies on different environments:&lt;/p&gt;





  











&lt;figure id=&#34;figure-ant-v2-environment&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shivanshmundra.github.io/img/ant_imi_rl.gif&#34; data-caption=&#34;Ant-v2 Environment&#34;&gt;


  &lt;img src=&#34;https://shivanshmundra.github.io/img/ant_imi_rl.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ant-v2 Environment
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-hopper-v2-environment&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shivanshmundra.github.io/img/hopper_imi_rl.gif&#34; data-caption=&#34;Hopper-v2 Environment&#34;&gt;


  &lt;img src=&#34;https://shivanshmundra.github.io/img/hopper_imi_rl.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Hopper-v2 Environment
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-humanoid-v2-environment&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shivanshmundra.github.io/img/human_imi_rl.gif&#34; data-caption=&#34;Humanoid-v2 Environment&#34;&gt;


  &lt;img src=&#34;https://shivanshmundra.github.io/img/human_imi_rl.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Humanoid-v2 Environment
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In all above simulations, on the left is trained policy using &lt;strong&gt;DAgger&lt;/strong&gt; algorithm and on right is expert policy which was used during training.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;other-algorithms&#34;&gt;Other Algorithms&lt;/h3&gt;
&lt;p&gt;All algorithms explained above are cateegorised as &amp;ldquo;basic&amp;rdquo; algorithms in Imitation learning. There are several other algorithms to discuss but I haven&amp;rsquo;t understood them completely.&lt;/p&gt;
&lt;p&gt;Some of them are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inverse Reinforcement Learning(IRL)&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learns the reward function from expert trajectories, then derives optimal policy.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s very expensive to run.&lt;/li&gt;
&lt;li&gt;Indirectly learns optimal policy from reward function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative Adversarial Imitation Learning(GAIL)&lt;/strong&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It learns the policy, not the reward function from data.&lt;/li&gt;
&lt;li&gt;Sometimes, it&amp;rsquo;s better than &amp;ldquo;expert&amp;rdquo; policy.&lt;/li&gt;
&lt;li&gt;Idea is inspired from Generative Adversarial Networks, where we need to approximate ground truth probability distribution.&lt;/li&gt;
&lt;li&gt;Here we need to approximate &amp;ldquo;expert&amp;rdquo; state-action distribution.&lt;/li&gt;
&lt;li&gt;The goal is to find a policy pi-theta such that the discriminator cannot distinguish between states following the pi-theta as opposed to those from pi-expert.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;So this was a detour of Imitation Learning, Hope you found it insightful! I would love to hear feedback, if any. ❤️&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c&#34;&gt;https://medium.com/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34;&gt;http://rail.eecs.berkeley.edu/deeprlcourse/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Shivanshmundra/CS285-DeepRL-Solutions/tree/master/hw1&#34;&gt;https://github.com/Shivanshmundra/CS285-DeepRL-Solutions/tree/master/hw1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hollygrimm.com/rl_gail&#34;&gt;https://hollygrimm.com/rl_gail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@sanketgujar95/generative-adversarial-imitation-learning-266f45634e60&#34;&gt;https://medium.com/@sanketgujar95/generative-adversarial-imitation-learning-266f45634e60&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@jonathan_hui/rl-imitation-learning-ac28116c02fc&#34;&gt;https://medium.com/@jonathan_hui/rl-imitation-learning-ac28116c02fc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
