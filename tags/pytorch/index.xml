<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PyTorch | Shivansh Mundra</title>
    <link>https://shivanshmundra.github.io/tags/pytorch/</link>
      <atom:link href="https://shivanshmundra.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <description>PyTorch</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Shivansh Mundra</copyright><lastBuildDate>Sat, 25 Apr 2020 11:59:39 +0530</lastBuildDate>
    <image>
      <url>https://shivanshmundra.github.io/img/icon-192.png</url>
      <title>PyTorch</title>
      <link>https://shivanshmundra.github.io/tags/pytorch/</link>
    </image>
    
    <item>
      <title>My Journey to Natural Language Processing</title>
      <link>https://shivanshmundra.github.io/paper_summary/intro_nlp/</link>
      <pubDate>Sat, 25 Apr 2020 11:59:39 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/intro_nlp/</guid>
      <description>&lt;h2 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h2&gt;
&lt;p&gt;I consider the reader to be familiar with normal machine learning terminology and methods. We here are going to deal with Supervised learning mostly.&lt;/p&gt;
&lt;p&gt;Since all machine learning models are mathematical function approximators, we can;t input a sentence to a mathematical model!&lt;/p&gt;
&lt;p&gt;There is nothing like &lt;code&gt;f(&#39;I am here to learn NLP&#39;)&lt;/code&gt;, one need to convert from our language to machine language so that it becomes something like &lt;code&gt;f([1, 0, 1, 2, 5 ..])&lt;/code&gt;, since this is the method computer is most familiar with. So first jumping to this conversion:&lt;/p&gt;
&lt;h3 id=&#34;conversion-of-text-to-mathematical-representation&#34;&gt;Conversion of text to mathematical representation&lt;/h3&gt;
&lt;h4 id=&#34;1-basic-approach-of-making-dictionary&#34;&gt;1. Basic Approach of making dictionary&lt;/h4&gt;
&lt;p&gt;Here you just create a dictionary or set of all available words present in data and represent each word/sentence with one hot vector. For example : Let&amp;rsquo;s say we have 8 words in our dictionary and sentence &lt;code&gt;I like banana&lt;/code&gt; is to be encoded, it would be something like &lt;code&gt;[1,0,0,0,1,0,0,1]&lt;/code&gt; where index representing &lt;code&gt;I, like and Banana&lt;/code&gt;. In this case, length of this vector would be fixed and equal to vocabulary of data.&lt;/p&gt;
&lt;p&gt;In this case, there is no sense of context here, &lt;code&gt;like&lt;/code&gt; can be used in different context yet same representation.  But duh! It&amp;rsquo;s the first method and supposed to be this dumb!&lt;/p&gt;
&lt;h4 id=&#34;2-tf-idf-representation&#34;&gt;2. TF-IDF Representation&lt;/h4&gt;
&lt;p&gt;Full form is : &lt;strong&gt;Term Frequency-Inverse Document Frequency&lt;/strong&gt; . The full form mentioned is self explanatory still we would want to get a feel. So here it is:&lt;/p&gt;
&lt;p&gt;Term-Frequency representation means number of times a word is occurring and represent in one hot vector by it&amp;rsquo;s frequency for ex &lt;code&gt;[1,2,0,0,3]&lt;/code&gt;. Now in this case, let&amp;rsquo;s say there is a common word &lt;code&gt;the&lt;/code&gt; which is occurring in most of corpus(another name for sentence!) but adds no specific meaning to understanding. So we take TF representation and multiply it with IDF which is $log(N/nw)$ where N is total number of document/corpus  and nw is number of corpus which contain that word. Smart thinking!&lt;/p&gt;
&lt;p&gt;We won&amp;rsquo;t use this in our supervised learning paradigm, we would just go with one hot vector and prepare embedding for our model.&lt;/p&gt;
&lt;h3 id=&#34;terminologies&#34;&gt;Terminologies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Corpus(singular- Corpura) : A sentence or document in general consisting of words.&lt;/li&gt;
&lt;li&gt;Tokenization: Splitting a corpus based on white spaces(in general) in between(for English specifically).
&lt;ul&gt;
&lt;li&gt;Say a sentence &lt;code&gt;I want to learn Tokenization :-)&lt;/code&gt; is to be tokenized.&lt;/li&gt;
&lt;li&gt;Tokenization would be &lt;code&gt;[&#39;I&#39;, &#39;want&#39;, &#39;to&#39;, &#39;learn&#39;, &#39;Tokenization&#39;, &#39;:-)&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;There are several libraries which use context and tokenize the corpus.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;N-gram tokens: Taking consecutive &lt;code&gt;n&lt;/code&gt; tokens from tokenized representation in contiguous manner.&lt;/li&gt;
&lt;li&gt;POS Tagging: Categorising tokens to parts of speech.&lt;/li&gt;
&lt;li&gt;Dependency Parsing: Extracting relationship/dependency between different tokens in sentence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;workflow-of-nlp-pipeline&#34;&gt;Workflow of NLP Pipeline&lt;/h3&gt;
&lt;p&gt;In every NLP data, there are three main components:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Vocabulary:&lt;/strong&gt; We want a bijection mapping where we map tokens to mathematical integers. We create a dictionary mapping tokens to indexes. This would also contain index to token mapping.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Vectorizer&lt;/strong&gt;: This method produces vectors from text/corpus using vocabulary above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Dataloader&lt;/strong&gt;: Traditional data science batchloader which yield data points in rolling fashion.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
