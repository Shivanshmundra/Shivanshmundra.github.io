<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CNN | Shivansh Mundra</title>
    <link>https://shivanshmundra.github.io/tags/cnn/</link>
      <atom:link href="https://shivanshmundra.github.io/tags/cnn/index.xml" rel="self" type="application/rss+xml" />
    <description>CNN</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Shivansh Mundra</copyright><lastBuildDate>Tue, 18 Aug 2020 14:24:26 +0530</lastBuildDate>
    <image>
      <url>https://shivanshmundra.github.io/img/icon-192.png</url>
      <title>CNN</title>
      <link>https://shivanshmundra.github.io/tags/cnn/</link>
    </image>
    
    <item>
      <title>Multi Modal Variational AutoEncoders</title>
      <link>https://shivanshmundra.github.io/paper_summary/mmvae/</link>
      <pubDate>Tue, 18 Aug 2020 14:24:26 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/mmvae/</guid>
      <description>&lt;h1 id=&#34;variational-mixture-of-experts-autoencoders-for-multi-modal-deep-generative-modelshttpsarxivorgpdf191103393pdf&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1911.03393.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Variational Mixture-of-Experts Autoencoders for Multi Modal Deep Generative Models&lt;/a&gt;&lt;/h1&gt;
&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;
&lt;p&gt;They characterize successful learning of such models as fulfilment of four criteria - i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818150052698.png&#34; alt=&#34;image-20200818150052698&#34;&gt;&lt;/p&gt;
&lt;p&gt;When we take all modalities(visual, linguistic and physical) into account, we can get a better understanding and representation of context, for ex. here bird.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818150435770.png&#34; alt=&#34;image-20200818150435770&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818150445361.png&#34; alt=&#34;image-20200818150445361&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Latent Factorization - There is some shared and private spaces across different modalities.&lt;/li&gt;
&lt;li&gt;Joint Generation - From union of both spaces we could generate different modalities. Basically, for example, text and image should be semantically same.&lt;/li&gt;
&lt;li&gt;Cross Generation - Model can generate data in one modality conditioned on some other modality. For ex. From text, generate an image.&lt;/li&gt;
&lt;li&gt;Synergy - Observing both modalities should enhance context understanding. For example observing image and text should improve in specificity of generation of image and text when taken alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818151150020.png&#34; alt=&#34;image-20200818151150020&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200818151230522.png&#34; alt=&#34;image-20200818151230522&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;related-works&#34;&gt;Related Works&lt;/h3&gt;
&lt;p&gt;There has been some work in the area of joint representations for example&lt;/p&gt;
&lt;p&gt;Suzuki et al. (2017) introduced the joint multimodal VAE (JMVAE) that learns shared representation with joint encoder qΦ(z | x1, x2). To handle missing data at test time, two unimodal encoders qΦ(z | x1) and qΦ(z | x2) are trained to match qΦ(z | x1, x2) with a KL constraint between them.&lt;/p&gt;
&lt;p&gt;However, they only focuses on one way transformation that too image-to-image and often require additional modelling components.&lt;/p&gt;
&lt;p&gt;More recently, Wu and Goodman (2018) introduced Product of Experts(PoE) over marginal posteriors, enabling cross model generations at test time without requiring additional inference networks and multi-stage training regimes.&lt;/p&gt;
&lt;h3 id=&#34;some-mathematics&#34;&gt;Some Mathematics&lt;/h3&gt;
&lt;p&gt;We denote modalities with m = 1,2,3&amp;hellip;&amp;hellip;M and latent representation as z.&lt;/p&gt;
&lt;p&gt;VAE is of form :  &lt;img src=&#34;image-20200818152456926.png&#34; alt=&#34;image-20200818152456926&#34;&gt;&lt;/p&gt;
&lt;p&gt;VAEs are parametrized by $\theta$ which is deep neural network.&lt;/p&gt;
&lt;p&gt;The objective of training VAEs is to maximise the marginal likelihood of
the data pΘ(x1:M). However, computing the evidence is intractable as it requires knowledge of the true joint posterior pΘ(z | x1:M). To tackle this, we approximate the true unknown posterior by a variational posterior qΦ(z | x1:M), which now allows optimising an evidence lower bound (ELBO) through stochastic gradient descent (SGD), with ELBO defined as&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200819173629938.png&#34; alt=&#34;image-20200819173629938&#34;&gt;&lt;/p&gt;
&lt;p&gt;They further used IWAE estimator to get a tighter bound and higher entropy.&lt;/p&gt;
&lt;p&gt;Here too, we are facing one issue, how to get joint posterior q$\phi$? One basic way is get a single encoder that takes all modalities as input. But that would mean, we need all modalities present at test time which won&amp;rsquo;t be true for cross modal generation.&lt;/p&gt;
&lt;p&gt;So instead they propose to factorise joint variational posterior as a combination of unimodal posteriors(weighted sum) :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;image-20200819174049519.png&#34; alt=&#34;image-20200819174049519&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When using Product-of-Experts, any one modality hold the power of veto.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For better understanding of VAEs both from Neural Network and probabilistic perspective, you can follow these links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ermongroup.github.io/cs228-notes/inference/variational/&#34;&gt;https://ermongroup.github.io/cs228-notes/inference/variational/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter19/readings/VI_Review.pdf&#34;&gt;https://www.cse.iitk.ac.in/users/piyush/courses/tpmi_winter19/readings/VI_Review.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&#34;&gt;https://jaan.io/what-is-variational-autoencoder-vae-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;As this is a new idea and theory based paper, they have done experiments on MNIST-SVHN data and Caltech Bird dataset(challenging). You can view results 
&lt;a href=&#34;https://github.com/iffsid/mmvae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; on all four characteristics mentioned above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Recurrent Multimodal Interaction for Referring Image Segmentation</title>
      <link>https://shivanshmundra.github.io/paper_summary/recurrent_multimodal/</link>
      <pubDate>Wed, 29 Apr 2020 00:17:44 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/recurrent_multimodal/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;The problem statement is to segment out visual object from natural language query. Semantics of that visual object are defined from NLP query. For ex. &lt;code&gt;Man in pink shirt&lt;/code&gt; in some image, we want to segment out corresponding region where any/all man is wearing pink shirt.&lt;/p&gt;
&lt;h2 id=&#34;related-works-and-inspiration&#34;&gt;Related Works and Inspiration&lt;/h2&gt;
&lt;p&gt;Right now this problem is addressed in academia by extracting out image features separately from Deep Convolution Neural Network and language features from LSTM cells. These features are then concatenated together to form a fused representation of image and query. This fused representation is somehow used to predict corresponding segmented region with trivial loss function.&lt;/p&gt;
&lt;p&gt;What authors feel is we, human don&amp;rsquo;t perceive things is similar manner like LSTM, we don&amp;rsquo;t remember everything after the last loop over word embeddings. In author words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, consider the expression “the man on the right wearing blue”. Without seeing an actual image, all information in the sentence needs to be remembered, meaning the sentence embedding needs to encode IS MAN, ON RIGHT, WEAR BLUE jointly. However, with the actual image available, the reasoning process can be decomposed as a sequential process, where the model first identifies all pixels that agree with IS MAN, then prunes out those that do not correspond with ON RIGHT, and finally suppresses those that do not agree with WEAR BLUE.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This task is also similar to visual question answering task where you need to generate answer(NLP) based on an image and a question. The difference only is, in VQA, we need sequence generation(in answer) while here we need to produce segmentation mask at once.&lt;/p&gt;
&lt;p&gt;Authors achieve this by applying LSTM in a convolution manner.&lt;/p&gt;
&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We propose a novel model, namely convolutional multimodal LSTM, to encode the sequential interactions between individual semantic, visual, and spatial information.&lt;/li&gt;
&lt;li&gt;We demonstrate the superior performance of the word to-image multimodal LSTM approach on benchmark datasets over the baseline model.&lt;/li&gt;
&lt;li&gt;We analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-toimage interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Basically they want to convert this referring image segmentation into an sequential process.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;rmi_model.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;rmi_np.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here concatenation of visual features and spatial features with language features is fed to &lt;code&gt;mLSTM&lt;/code&gt; cell as an input and &lt;code&gt;mLSTM&lt;/code&gt; cell produces an output for same. SInce mLSTM receives multimodal data, we can understand this as a 1x1 convolution applied to fused features .&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The same mLSTM operation is shared for all image locations. This is equivalent to treating the mLSTM as a 1×1 convolution over the feature map of size W 0 × H0 × (DI + DS + 8). In other words, this is a convolutional LSTM that shares weights both across spatial location and time step.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;result_rmi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;my-takeaways&#34;&gt;My Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This idea of treating this problem as sequential problem is innovative and having an mLSTM cell to encode both visual and linguistic features is also good. This gives model ability to forget all those pixel which defy correspondence initially.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;My guts are that this kind of model would work good even where there are small objects because at each time step there would be a reduction/change of probable pixels for segmentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I still believe that this essentially is fusing of features from image and language just more diffused!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I want to study good papers on VQA as they can provide me with good literature and insights on this task.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Xception: Deep Learning with Depthwise Separable Convolutions</title>
      <link>https://shivanshmundra.github.io/paper_summary/xceptionnet/</link>
      <pubDate>Wed, 22 Apr 2020 21:19:38 +0530</pubDate>
      <guid>https://shivanshmundra.github.io/paper_summary/xceptionnet/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1610.02357&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;previous-works-and-inspiration&#34;&gt;Previous Works and Inspiration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;While Inception modules are conceptually similar to convolutions, the empirically appear to be capable of learning richer representations with less parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A convolution layer attempts to learn filters in a 3D space, with 2 spatial dimensions (width and height) and a channel dimension; thus a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In Inception module, we introduce 1x1 convolutions instead of 3 or 5 to extract fewer channels and focus on spatial information. After that mapping all correlations in these smaller 3D spaces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In theory, cross channel correlations and spatial correlations are decoupled in Inception module.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;inception_archi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;separable-convolution-layer&#34;&gt;Separable Convolution Layer&lt;/h3&gt;
&lt;p&gt;So essentially the idea of inception of separating cross channel mapping and spatial mapping was already there as a layer in Tensorflow as &amp;ldquo;Separable Conv&amp;rdquo; Layer.&lt;/p&gt;
&lt;p&gt;Main difference between this and Inception version is :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The order of the operations: depthwise separable convolutions as usually  implemented (e.g. in TensorFlow) perform first channel-wise spatial convolution and then perform 1x1 convolution, whereas Inception performs the 1x1 convolution first.(doesn&amp;rsquo;t have a difference as both are stacked)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The presence or absence of a non-linearity after the first operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise separable convolutions are usually implemented without non-linearities.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Author say that we can totally decouple cross channel correlations and spatial correlations in the feature maps of convolution neural network.&lt;/li&gt;
&lt;li&gt;Parameters are almost same in Inception and Xception model while Xception outperforms by a large margin on a dataset with large number of classes(17k).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;xception_archi.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rest all things are same, just replacing inception module by depthwise separable conv layer block also making the architecture simple.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;takeaway&#34;&gt;Takeaway&lt;/h2&gt;
&lt;p&gt;Idea of decoupling of channel dimension and spatial feature mapping from convolution kernel is great! So deciphering it mathematically:&lt;/p&gt;
&lt;p&gt;We know each channel in Convolution block represents some high level feature say in human, each channel would map a body part(just for example!). Then this idea of  depthwise conv proposes that in process of identifying one body part don&amp;rsquo;t process info from other body part info, it would disturb the signal in processing.&lt;/p&gt;
&lt;p&gt;After we get good signal info body part, then we can use info from others to get high level part info.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
